{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 658267,
     "sourceType": "datasetVersion",
     "datasetId": 277323
    },
    {
     "sourceId": 13086054,
     "sourceType": "datasetVersion",
     "datasetId": 8288443
    }
   ],
   "dockerImageVersionId": 31153,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "source": "<a href=\"https://www.kaggle.com/code/kescovar/resnet50-few-shot-learning-31102025?scriptVersionId=270883266\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>",
   "metadata": {},
   "cell_type": "markdown"
  },
  {
   "cell_type": "code",
   "source": "import albumentations as A",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:47.678147Z",
     "iopub.execute_input": "2025-10-26T00:46:47.678931Z",
     "iopub.status.idle": "2025-10-26T00:46:47.682239Z",
     "shell.execute_reply.started": "2025-10-26T00:46:47.678908Z",
     "shell.execute_reply": "2025-10-26T00:46:47.681406Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install albumentations",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:47.707891Z",
     "iopub.execute_input": "2025-10-26T00:46:47.708126Z",
     "iopub.status.idle": "2025-10-26T00:46:50.985488Z",
     "shell.execute_reply.started": "2025-10-26T00:46:47.708107Z",
     "shell.execute_reply": "2025-10-26T00:46:50.984416Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd.variable as Variable\n",
    "\n",
    "from glob import glob\n",
    "from math import sqrt\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian noise layer for data augmentation during training.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size for noise tensor\n",
    "        input_shape: Shape of input images (default: (3, 84, 84))\n",
    "        std: Standard deviation of noise (default: 0.05)\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, input_shape=(3, 84, 84), std=0.05):\n",
    "        super(GaussianNoise, self).__init__()\n",
    "        self.shape = (batch_size,) + input_shape\n",
    "        self.noise = Variable(torch.zeros(self.shape).cuda())\n",
    "        self.std = std\n",
    "\n",
    "    def forward(self, x, std=0.15):\n",
    "        noise = Variable(torch.zeros(x.shape).cuda())\n",
    "        noise = noise.data.normal_(0, std=std)\n",
    "        return x + noise\n",
    "\n",
    "\n",
    "def set_gpu(x):\n",
    "    \"\"\"\n",
    "    Set visible GPU device.\n",
    "\n",
    "    Args:\n",
    "        x: GPU device ID (as string)\n",
    "    \"\"\"\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = x\n",
    "    print('using gpu:', x)\n",
    "\n",
    "\n",
    "def clone(tensor):\n",
    "    \"\"\"\n",
    "    Clone a tensor preserving its computational graph.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor to clone\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Cloned tensor\n",
    "    \"\"\"\n",
    "    cloned = tensor.clone()#tensor.detach().clone()\n",
    "    # cloned.requires_grad = tensor.requires_grad\n",
    "    # if tensor.grad is not None:\n",
    "    #     cloned.grad = clone(tensor.grad)\n",
    "    return cloned\n",
    "\n",
    "def clone_state_dict(state_dict):\n",
    "    \"\"\"Clone a state_dict. If state_dict is from a ``torch.nn.Module``, use ``keep_vars=True``.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (OrderedDict): the state_dict to clone. Assumes state_dict is not detached from model state.\n",
    "    \"\"\"\n",
    "    return OrderedDict([(name, clone(param)) for name, param in state_dict.items()])\n",
    "\n",
    "def ensure_path(path):\n",
    "    if os.path.exists(path):\n",
    "        if input('{} exists, remove? ([y]/n)'.format(path)) != 'n':\n",
    "            shutil.rmtree(path)\n",
    "            os.mkdir(path)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "\n",
    "class Averager:\n",
    "    \"\"\"Running average tracker for metrics.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.v = 0\n",
    "\n",
    "    def add(self, x: float) -> None:\n",
    "        self.v = (self.v * self.n + x) / (self.n + 1)\n",
    "        self.n += 1\n",
    "\n",
    "    def item(self) -> float:\n",
    "        return self.v\n",
    "\n",
    "def count_acc(logits: torch.Tensor, label: torch.Tensor) -> float:\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    return (pred == label).type(torch.cuda.FloatTensor).mean().item()\n",
    "\n",
    "def dot_metric(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mm(a, b.t())\n",
    "\n",
    "def euclidean_metric(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "    n = a.shape[0]\n",
    "    m = b.shape[0]\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    logits = -((a - b)**2).sum(dim=2)\n",
    "    return logits\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Simple timer for tracking epoch duration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.o = time.time()\n",
    "\n",
    "    def measure(self, p: float = 1) -> str:\n",
    "        x = (time.time() - self.o) / p\n",
    "        x = int(x)\n",
    "        if x >= 3600:\n",
    "            return '{:.1f}h'.format(x / 3600)\n",
    "        if x >= 60:\n",
    "            return '{}m'.format(round(x / 60))\n",
    "        return '{}s'.format(x)\n",
    "\n",
    "_utils_pp = pprint.PrettyPrinter()\n",
    "def pprint(x):\n",
    "    _utils_pp.pprint(x)\n",
    "\n",
    "def l2_loss(pred: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
    "    return ((pred - label)**2).sum() / len(pred) / 2\n",
    "\n",
    "def set_protocol(data_path, protocol, test_protocol, subset=None):\n",
    "    \"\"\"\n",
    "    Set up data paths based on protocol configuration.\n",
    "\n",
    "    Args:\n",
    "        data_path: Base path to dataset\n",
    "        protocol: Training protocol ('p1', 'p2', 'p3', 'p4')\n",
    "        test_protocol: Testing protocol ('p1', 'p2', 'p3', 'p4')\n",
    "        subset: Optional subset name\n",
    "\n",
    "    Returns:\n",
    "        train: List of training data paths\n",
    "        val: List of validation data paths\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    val = []\n",
    "    all_set = ['shn', 'hon', 'clv', 'clk', 'gls', 'scl', 'sci', 'nat', 'shx', 'rel']\n",
    "    if subset is not None:\n",
    "        train.append(data_path + '/crops_' + subset + '/')\n",
    "        val.append(data_path + '/crops_' + subset + '/')\n",
    "    if protocol == 'p1':\n",
    "        for i in range(3):\n",
    "            train.append(data_path + '/crops_' + all_set[i])\n",
    "    elif protocol == 'p2':\n",
    "        for i in range(3, 6):\n",
    "            train.append(data_path + '/crops_' + all_set[i])\n",
    "    elif protocol == 'p3':\n",
    "        for i in range(6, 8):\n",
    "            train.append(data_path + '/crops_' + all_set[i])\n",
    "    elif protocol == 'p4':\n",
    "        for i in range(8, 10):\n",
    "            train.append(data_path + '/crops_' + all_set[i])\n",
    "\n",
    "    if test_protocol == 'p1':\n",
    "        for i in range(3):\n",
    "            val.append(data_path + '/crops_' + all_set[i])\n",
    "    elif test_protocol == 'p2':\n",
    "        for i in range(3, 6):\n",
    "            val.append(data_path + '/crops_' + all_set[i])\n",
    "    elif test_protocol == 'p3':\n",
    "        for i in range(6, 8):\n",
    "            val.append(data_path + '/crops_' + all_set[i])\n",
    "    elif test_protocol == 'p4':\n",
    "        for i in range(8, 10):\n",
    "            val.append(data_path + '/crops_' + all_set[i])\n",
    "    return train, val\n",
    "\n",
    "def flip(x, dim):\n",
    "    \"\"\"\n",
    "    Flip tensor along specified dimension.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        dim: Dimension to flip along (can be negative)\n",
    "\n",
    "    Returns:\n",
    "        Flipped tensor\n",
    "    \"\"\"\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    return x[tuple(slice(None, None) if i != dim\n",
    "             else torch.arange(x.size(i)-1, -1, -1).long()\n",
    "             for i in range(x.dim()))]\n",
    "\n",
    "def perturb(data):\n",
    "    \"\"\"\n",
    "    Apply random geometric perturbation for data augmentation.\n",
    "\n",
    "    Randomly applies one of: horizontal flip, vertical flip,\n",
    "    90-degree rotation, or 270-degree rotation.\n",
    "\n",
    "    Args:\n",
    "        data: Input tensor batch\n",
    "\n",
    "    Returns:\n",
    "        Concatenated original and perturbed data\n",
    "    \"\"\"\n",
    "    randno = np.random.randint(0, 5)\n",
    "    if randno == 1:\n",
    "        return torch.cat((data, data.flip(3)), dim=0)\n",
    "    elif randno == 2: #180\n",
    "        return torch.cat((data, data.flip(2)), dim=0)\n",
    "    elif randno == 3: #90\n",
    "        return torch.cat((data, data.transpose(2,3)), dim=0)\n",
    "    else:\n",
    "        return torch.cat((data, data.transpose(2, 3).flip(3)), dim=0)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:50.987219Z",
     "iopub.execute_input": "2025-10-26T00:46:50.987567Z",
     "iopub.status.idle": "2025-10-26T00:46:51.008545Z",
     "shell.execute_reply.started": "2025-10-26T00:46:50.987544Z",
     "shell.execute_reply": "2025-10-26T00:46:51.007744Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Data Manager",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "cd '/kaggle/working/'",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:51.009269Z",
     "iopub.execute_input": "2025-10-26T00:46:51.009467Z",
     "iopub.status.idle": "2025-10-26T00:46:51.031857Z",
     "shell.execute_reply.started": "2025-10-26T00:46:51.009452Z",
     "shell.execute_reply": "2025-10-26T00:46:51.030865Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# --- Step 1: Gather All Data ---\nprint(\"Scanning all image paths...\")\nall_paths_raw = glob(\"/kaggle/input/plantvillage-dataset/color/*/*.JPG\")\n\n# Filter out problematic Pepper classes\nall_paths = []\nfor path in all_paths_raw:\n    if 'Pepper,_bell___healthy' in path or 'Pepper,_bell___Bacterial_spot' in path:\n        continue  # Skip these classes\n    label = path.split('/')[-2]  # Extract class name from path\n    all_paths.append([path, label])\n\nprint(f\"Found {len(all_paths)} total images after filtering out Pepper classes.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:51.033848Z",
     "iopub.execute_input": "2025-10-26T00:46:51.034082Z",
     "iopub.status.idle": "2025-10-26T00:46:51.228887Z",
     "shell.execute_reply.started": "2025-10-26T00:46:51.034066Z",
     "shell.execute_reply": "2025-10-26T00:46:51.228171Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os.path as osp\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# Strong augmentation pipeline for training\n",
    "# Includes spatial transforms, color jittering, and normalization\n",
    "leaf_train_augmentation = A.Compose([\n",
    "    # spatial\n",
    "    A.RandomResizedCrop(size=(84, 84), scale=(0.7, 1.0), ratio=(0.75, 1.33), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.Rotate(limit=180, p=0.7, border_mode=0),\n",
    "    A.Perspective(scale=(0.05, 0.1), p=0.4),\n",
    "\n",
    "    # color / lighting\n",
    "    A.OneOf([\n",
    "        # controlled color jitter\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=1.0),\n",
    "        # HSV shifts: explicit shift limits (degrees for hue, percent for saturation/value)\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1.0),\n",
    "        # random brightness/contrast\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "    ], p=0.9),\n",
    "\n",
    "    # Apply either gamma, CLAHE or tone curve (kept), but not all every time\n",
    "    A.OneOf([\n",
    "        A.RandomGamma(gamma_limit=(80,120), p=1.0),\n",
    "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=1.0),\n",
    "        A.RandomToneCurve(p=1.0),\n",
    "    ], p=0.6),\n",
    "\n",
    "    # final normalization + tensor conversion\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Conservative augmentation pipeline\n",
    "# Preserves important features while adding realistic variations\n",
    "leaf_smart_augmentation = A.Compose([\n",
    "    # Conservative spatial transforms\n",
    "    A.Resize(92, 92),  # Slightly larger then crop\n",
    "    A.RandomCrop(84, 84, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.3),  # Small rotations\n",
    "    \n",
    "    # Color variations that mimic real-world conditions\n",
    "    A.OneOf([\n",
    "        # Lighting changes\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
    "        # Color temperature variations\n",
    "        A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=1.0),\n",
    "    ], p=0.6),\n",
    "    \n",
    "    # Mild noise for sensor variations\n",
    "    A.GaussNoise(var_limit=(5.0, 15.0), p=0.1),\n",
    "    \n",
    "    # Focus on preserving texture details\n",
    "    A.Sharpen(alpha=(0.05, 0.1), lightness=(0.8, 1.0), p=0.1),  # Very mild sharpening\n",
    "    \n",
    "    # Normalize\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Validation/test augmentation pipeline\n",
    "# Minimal augmentation - only resize and normalize\n",
    "leaf_val_augmentation = A.Compose([\n",
    "    A.Resize(96, 96),  # Slightly larger\n",
    "    A.CenterCrop(84, 84),  # Clean center crop\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Fine-tuning augmentation pipeline\n",
    "# Moderate augmentation for fine-tuning phase\n",
    "leaf_finetune_augmentation = A.Compose([\n",
    "    A.RandomResizedCrop(size=(84, 84), scale=(0.85, 1.0), ratio=(0.9, 1.1), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=45, p=0.5, border_mode=0),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.7),\n",
    "    A.GaussianBlur(blur_limit=(3, 5), p=0.15),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# UPDATED DATASET CLASS\n",
    "# ========================================\n",
    "class UiSmell(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for PlantVillage disease classification.\n",
    "\n",
    "    Loads images and applies augmentation pipelines using Albumentations.\n",
    "    Supports both training (with augmentation) and validation/test modes.\n",
    "\n",
    "    Args:\n",
    "        setname: Dataset split name ('train', 'val', or 'test')\n",
    "        img_path: Root path to image directory\n",
    "        is_aug: Whether to apply training augmentations (default: False)\n",
    "    \"\"\"\n",
    "    def __init__(self, setname, img_path, is_aug=False):\n",
    "        csv_path = osp.join('/kaggle/working/materials/', setname + '.csv')\n",
    "        try:\n",
    "            with open(csv_path, 'r') as f:\n",
    "                lines = [x.strip() for x in f.readlines()][1:]\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "        self.is_aug = is_aug\n",
    "        self.img_path = img_path\n",
    "\n",
    "        data, label = [], []\n",
    "        label_map, label_counter = {}, 0\n",
    "\n",
    "        for line in lines:\n",
    "            name, lbl = line.split(',', 1)\n",
    "            lbl_clean = lbl.strip()\n",
    "            if lbl_clean not in label_map:\n",
    "                label_map[lbl_clean] = label_counter\n",
    "                label_counter += 1\n",
    "            path = osp.join(img_path, name)\n",
    "            data.append(path)\n",
    "            label.append(label_map[lbl_clean])\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.label_map = label_map\n",
    "\n",
    "        if is_aug:\n",
    "            self.transform = leaf_train_augmentation\n",
    "            print(f\"\u2705 Using STRONG leaf augmentation for {setname}\")\n",
    "        else:\n",
    "            self.transform = leaf_val_augmentation\n",
    "            print(f\"\u2705 Using VALIDATION augmentation for {setname}\")\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} samples with {len(self.label_map)} classes for {setname}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        # PIL -> NumPy (H,W,C)\n",
    "        image = np.array(Image.open(path).convert('RGB'))\n",
    "\n",
    "        # Apply albumentations\n",
    "        augmented = self.transform(image=image)\n",
    "        image = augmented['image']  # torch.Tensor from ToTensorV2\n",
    "\n",
    "        # Ensure float32 (should already be)\n",
    "        if not torch.is_floating_point(image):\n",
    "            image = image.float()\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:51.229596Z",
     "iopub.execute_input": "2025-10-26T00:46:51.229817Z",
     "iopub.status.idle": "2025-10-26T00:46:51.261237Z",
     "shell.execute_reply.started": "2025-10-26T00:46:51.2298Z",
     "shell.execute_reply": "2025-10-26T00:46:51.260554Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# import pandas as pd\n# df = pd.read_csv(\"/kaggle/working/materials/train.csv\")\n# df.label.value_counts()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:51.262099Z",
     "iopub.execute_input": "2025-10-26T00:46:51.262776Z",
     "iopub.status.idle": "2025-10-26T00:46:51.265804Z",
     "shell.execute_reply.started": "2025-10-26T00:46:51.262756Z",
     "shell.execute_reply": "2025-10-26T00:46:51.265164Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Sampler",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CategoriesSampler():\n",
    "    \"\"\"\n",
    "    Episodic sampler for few-shot learning.\n",
    "\n",
    "    Samples N-way K-shot episodes from the dataset.\n",
    "\n",
    "    Args:\n",
    "        label: List of all labels in dataset\n",
    "        n_batch: Number of episodes to generate\n",
    "        n_cls: Number of classes per episode (N-way)\n",
    "        n_per: Number of samples per class (K-shot + query)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, label, n_batch, n_cls, n_per):\n",
    "        self.n_batch = n_batch\n",
    "        self.n_cls = n_cls\n",
    "        self.n_per = n_per\n",
    "\n",
    "        label = np.array(label)\n",
    "        self.m_ind = []\n",
    "        for i in range(max(label) + 1):\n",
    "            ind = np.argwhere(label == i).reshape(-1)\n",
    "            ind = torch.from_numpy(ind)\n",
    "            if len(ind) >= n_per:  # Ensure enough samples per class\n",
    "                self.m_ind.append(ind)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batch\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if len(self.m_ind) < self.n_cls:\n",
    "            raise ValueError(f\"Not enough classes ({len(self.m_ind)}) for {self.n_cls}-way sampling\")\n",
    "\n",
    "        for i_batch in range(self.n_batch):\n",
    "            batch = []\n",
    "            classes = torch.randperm(len(self.m_ind))[:self.n_cls]\n",
    "            for c in classes:\n",
    "                l = self.m_ind[c]\n",
    "                pos = torch.randperm(len(l))[:self.n_per]\n",
    "                batch.append(l[pos])\n",
    "            batch = torch.stack(batch).t().reshape(-1)\n",
    "            #for i in range(1000):\n",
    "            yield batch"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:51.266553Z",
     "iopub.execute_input": "2025-10-26T00:46:51.266785Z",
     "iopub.status.idle": "2025-10-26T00:46:51.281413Z",
     "shell.execute_reply.started": "2025-10-26T00:46:51.26676Z",
     "shell.execute_reply": "2025-10-26T00:46:51.28072Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Convnet",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install torchsummary",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:51.282214Z",
     "iopub.execute_input": "2025-10-26T00:46:51.282442Z",
     "iopub.status.idle": "2025-10-26T00:46:54.462493Z",
     "shell.execute_reply.started": "2025-10-26T00:46:51.282426Z",
     "shell.execute_reply": "2025-10-26T00:46:54.461633Z"
    },
    "trusted": true,
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Duplicate removed - CategoriesSampler already defined above\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# \n",
    "# \n",
    "# class CategoriesSampler():\n",
    "# \n",
    "#     def __init__(self, label, n_batch, n_cls, n_per):\n",
    "#         self.n_batch = n_batch\n",
    "#         self.n_cls = n_cls\n",
    "#         self.n_per = n_per\n",
    "# \n",
    "#         label = np.array(label)\n",
    "#         self.m_ind = []\n",
    "#         for i in range(max(label) + 1):\n",
    "#             ind = np.argwhere(label == i).reshape(-1)\n",
    "#             ind = torch.from_numpy(ind)\n",
    "#             if len(ind) >= n_per:  # Ensure enough samples per class\n",
    "#                 self.m_ind.append(ind)\n",
    "# \n",
    "#     def __len__(self):\n",
    "#         return self.n_batch\n",
    "#     \n",
    "#     def __iter__(self):\n",
    "#         for i_batch in range(self.n_batch):\n",
    "#             batch = []\n",
    "#             classes = torch.randperm(len(self.m_ind))[:self.n_cls]\n",
    "#             for c in classes:\n",
    "#                 l = self.m_ind[c]\n",
    "#                 pos = torch.randperm(len(l))[:self.n_per]\n",
    "#                 batch.append(l[pos])\n",
    "#             batch = torch.stack(batch).t().reshape(-1)\n",
    "#             #for i in range(1000):\n",
    "#             yield batch"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:54.463617Z",
     "iopub.execute_input": "2025-10-26T00:46:54.463957Z",
     "iopub.status.idle": "2025-10-26T00:46:54.47121Z",
     "shell.execute_reply.started": "2025-10-26T00:46:54.463924Z",
     "shell.execute_reply": "2025-10-26T00:46:54.47054Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    \"\"\"\n",
    "    Create a convolutional block with BatchNorm, ReLU, and MaxPool.\n",
    "\n",
    "    Args:\n",
    "        in_channels: Number of input channels\n",
    "        out_channels: Number of output channels\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: Convolutional block\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        # nn.Hardswish(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 4-layer convolutional network baseline.\n",
    "\n",
    "    Args:\n",
    "        x_dim: Input image channels (default: 3)\n",
    "        hid_dim: Hidden layer dimension (default: 64)\n",
    "        z_dim: Output embedding dimension (default: 64)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(x_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, z_dim),\n",
    "        )\n",
    "        self.out_channels = 1600\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "import torchvision.models as models\n",
    "    \"\"\"\n",
    "    ResNet50-based encoder for Few-Shot Learning with Deep Subspace Networks.\n",
    "\n",
    "    Uses pretrained ResNet50 as backbone with selective layer fine-tuning.\n",
    "    Projects features into a lower-dimensional embedding space for metric learning.\n",
    "\n",
    "    Args:\n",
    "        hid_dim: Hidden dimension size (default: 512)\n",
    "        z_dim: Final embedding dimension (default: 256)\n",
    "        pretrained: Use ImageNet pretrained weights (default: True)\n",
    "        freeze_early_layers: Freeze layers before layer4 (default: True)\n",
    "        use_batchnorm: Add batch normalization in projector (default: True)\n",
    "        normalize_embeddings: L2 normalize final embeddings (default: True)\n",
    "        weight_decay: L2 regularization strength (default: 1e-4)\n",
    "    \"\"\"\n",
    "\n",
    "#     def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n",
    "#         super().__init__()\n",
    "#         self.encoder = models.resnet50(pretrained=True)\n",
    "#         self.out_channels = 1600\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.encoder(x)\n",
    "#         return x.view(x.size(0), -1)\n",
    "\n",
    "class Resnet50(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet50-based encoder for Few-Shot Learning with Deep Subspace Networks.\n",
    "\n",
    "    Uses pretrained ResNet50 as backbone with selective layer fine-tuning.\n",
    "    Projects features into a lower-dimensional embedding space for metric learning.\n",
    "\n",
    "    Args:\n",
    "        hid_dim: Hidden dimension size (default: 512)\n",
    "        z_dim: Final embedding dimension (default: 256)\n",
    "        pretrained: Use ImageNet pretrained weights (default: True)\n",
    "        freeze_early_layers: Freeze layers before layer4 (default: True)\n",
    "        use_batchnorm: Add batch normalization in projector (default: True)\n",
    "        normalize_embeddings: L2 normalize final embeddings (default: True)\n",
    "        weight_decay: L2 regularization strength (default: 1e-4)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 hid_dim=512, \n",
    "                 z_dim=256, \n",
    "                 pretrained=True,\n",
    "                 freeze_early_layers=True,\n",
    "                 use_batchnorm=True,\n",
    "                 normalize_embeddings=True,\n",
    "                 weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.encoder = models.resnet50(pretrained=pretrained)\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        \n",
    "        if freeze_early_layers:\n",
    "            for name, param in self.encoder.named_parameters():\n",
    "                if not ('layer4' in name or 'fc' in name):\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        layers = [nn.Linear(2048, hid_dim)]\n",
    "        \n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(hid_dim))\n",
    "        \n",
    "        layers.extend([\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, z_dim)\n",
    "        ])\n",
    "        \n",
    "        self.projector = nn.Sequential(*layers)\n",
    "        self.normalize_embeddings = normalize_embeddings\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.encoder(x)\n",
    "        embedding = self.projector(feat)\n",
    "        \n",
    "        if self.normalize_embeddings:\n",
    "            embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:54.473988Z",
     "iopub.execute_input": "2025-10-26T00:46:54.474365Z",
     "iopub.status.idle": "2025-10-26T00:46:54.488744Z",
     "shell.execute_reply.started": "2025-10-26T00:46:54.474346Z",
     "shell.execute_reply": "2025-10-26T00:46:54.487967Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Classifier",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Subspace_Projection(nn.Module):\n",
    "    \"\"\"\n",
    "    Subspace Projection module for Few-Shot Learning.\n",
    "\n",
    "    Creates class-specific subspaces from support set features using SVD,\n",
    "    then measures query distances to these subspaces for classification.\n",
    "\n",
    "    Args:\n",
    "        num_dim: Dimension of the subspace (default: 2)\n",
    "        debug: Enable debug printing (default: False)\n",
    "        eps: Epsilon for numerical stability (default: 1e-8)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_dim=2, debug=False, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.num_dim = num_dim\n",
    "        self.debug = debug\n",
    "        self.eps = eps  # Better numerical stability\n",
    "\n",
    "    def create_subspace(self, supportset_features, class_size, sample_size):\n",
    "        \"\"\"\n",
    "        Create class-specific subspaces using SVD on support set features.\n",
    "\n",
    "        Args:\n",
    "            supportset_features: Features from support set [class_size, sample_size, feature_dim]\n",
    "            class_size: Number of classes in the episode\n",
    "            sample_size: Number of samples per class (shot)\n",
    "\n",
    "        Returns:\n",
    "            all_hyper_planes: Subspace bases for each class [class_size, feature_dim, num_dim]\n",
    "            means: Mean vectors for each class [class_size, feature_dim]\n",
    "        \"\"\"\n",
    "        # Add validation\n",
    "        if sample_size < self.num_dim + 1:\n",
    "            self.num_dim = sample_size - 1\n",
    "            print(f\"Warning: Reduced subspace dim to {self.num_dim}\")\n",
    "        all_hyper_planes = []\n",
    "        means = []\n",
    "        for ii in range(class_size):\n",
    "            all_support = supportset_features[ii]\n",
    "            mean_vec = torch.mean(all_support, dim=0)\n",
    "            means.append(mean_vec)\n",
    "            centered = all_support - mean_vec.unsqueeze(0)\n",
    "            uu, s, v = torch.svd(centered.transpose(0, 1).double(), some=False)\n",
    "            uu = uu.float()\n",
    "            all_hyper_planes.append(uu[:, :self.num_dim])  # limit dimension!\n",
    "\n",
    "        all_hyper_planes = torch.stack(all_hyper_planes, dim=0)\n",
    "        means = torch.stack(means, dim=0)\n",
    "        return all_hyper_planes, means\n",
    "\n",
    "    def projection_metric(self, target_features, hyperplanes, mu):\n",
    "        \"\"\"\n",
    "        Compute similarity scores by projecting query features onto class subspaces.\n",
    "\n",
    "        Args:\n",
    "            target_features: Query features to classify [batch_size, feature_dim]\n",
    "            hyperplanes: Class subspace bases [class_size, feature_dim, num_dim]\n",
    "            mu: Class mean vectors [class_size, feature_dim]\n",
    "\n",
    "        Returns:\n",
    "            similarities: Negative distances to each class subspace [batch_size, class_size]\n",
    "            discriminative_loss: Regularization term encouraging orthogonal subspaces\n",
    "        \"\"\"\n",
    "        eps = 1e-12\n",
    "        device = target_features.device\n",
    "        batch_size = target_features.shape[0]\n",
    "        class_size = hyperplanes.shape[0]\n",
    "\n",
    "        similarities = []\n",
    "        discriminative_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        for j in range(class_size):\n",
    "            h_plane_j = hyperplanes[j].unsqueeze(0).repeat(batch_size, 1, 1).to(device)\n",
    "            tf_centered = (target_features - mu[j].expand_as(target_features)).unsqueeze(-1)\n",
    "            proj = torch.bmm(h_plane_j, torch.bmm(h_plane_j.transpose(1, 2), tf_centered))\n",
    "            proj = torch.squeeze(proj, -1) + mu[j].unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "            diff = target_features - proj\n",
    "            query_loss = -torch.sqrt(torch.sum(diff * diff, dim=-1) + eps)\n",
    "            similarities.append(query_loss)\n",
    "\n",
    "            # discriminative term (reduced)\n",
    "            for k in range(class_size):\n",
    "                if j != k:\n",
    "                    temp = torch.mm(hyperplanes[j].T, hyperplanes[k])\n",
    "                    discriminative_loss += torch.sum(temp * temp)\n",
    "\n",
    "        similarities = torch.stack(similarities, dim=1).to(device)\n",
    "        class_size = hyperplanes.shape[0]\n",
    "        discriminative_loss = discriminative_loss / (class_size * (class_size - 1) + 1e-6)\n",
    "        similarities = similarities / similarities.std(dim=1, keepdim=True).clamp_min(1e-6)\n",
    "\n",
    "        # ---- DEBUG ----\n",
    "        if self.debug:\n",
    "            print(\"[DEBUG] projection_metric DEBUG:\")\n",
    "            print(f\"  target_features: mean={target_features.mean():.6f}, std={target_features.std():.6f}\")\n",
    "            print(f\"  hyperplanes: mean={hyperplanes.mean():.6f}, std={hyperplanes.std():.6f}\")\n",
    "            print(f\"  similarities: mean={similarities.mean():.6f}, std={similarities.std():.6f}, min={similarities.min():.6f}, max={similarities.max():.6f}\")\n",
    "            print(f\"  discriminative_loss: {discriminative_loss.item():.6f}\")\n",
    "\n",
    "        return similarities, discriminative_loss"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:54.48945Z",
     "iopub.execute_input": "2025-10-26T00:46:54.489723Z",
     "iopub.status.idle": "2025-10-26T00:46:54.509245Z",
     "shell.execute_reply.started": "2025-10-26T00:46:54.489702Z",
     "shell.execute_reply": "2025-10-26T00:46:54.508501Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Training",
   "metadata": {
    "editable": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# SET MODEL NAME\n",
    "modelname = 'Resnet50'\n",
    "\n",
    "args = {}\n",
    "args['num_sampler'] = 500\n",
    "args['max-epoch'] = 25\n",
    "args['save-epoch'] = 5\n",
    "args['shot'] = 5\n",
    "args['query'] = 5\n",
    "args['train-way'] = 5\n",
    "args['test-way'] = 5\n",
    "args['data-path'] = '/kaggle/input/plantvillage-dataset/color/'\n",
    "args['gpu'] = '0'\n",
    "args['lamb'] = 0.5\n",
    "args['lr'] = 1e-4\n",
    "args['weight_decay'] = 1e-4\n",
    "args['subspace-dim'] = args['shot']-1\n",
    "set_gpu(args['gpu'])\n",
    "\n",
    "txt = str(datetime.now())\n",
    "txt = '_'.join([modelname+f\"-{args['shot']}-{args['lamb']}-{args['lr']}-{args['num_sampler']}\", txt[:4], txt[5:7], txt[8:10], txt[11:13], txt[14:16]])\n",
    "\n",
    "args['save-path'] = '/kaggle/working/save/'+txt\n",
    "\n",
    "# MODEL BUILDER\n",
    "# Initialize ResNet50 with default hyperparameters\n",
    "# The model uses pretrained ImageNet weights with selective fine-tuning\n",
    "model = {\n",
    "    'Resnet50': Resnet50(),\n",
    "}[modelname].cuda()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:54.509993Z",
     "iopub.execute_input": "2025-10-26T00:46:54.510264Z",
     "iopub.status.idle": "2025-10-26T00:46:55.039817Z",
     "shell.execute_reply.started": "2025-10-26T00:46:54.510244Z",
     "shell.execute_reply": "2025-10-26T00:46:55.039197Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport csv\nfrom glob import glob\nimport random\nfrom collections import Counter\nimport re",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.040655Z",
     "iopub.execute_input": "2025-10-26T00:46:55.040871Z",
     "iopub.status.idle": "2025-10-26T00:46:55.044503Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.040857Z",
     "shell.execute_reply": "2025-10-26T00:46:55.043916Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Step 1: Gather All Data ---\n",
    "print(\"Scanning all image paths...\")\n",
    "all_paths_raw = glob(\"/kaggle/input/plantvillage-dataset/color/*/*.JPG\")\n",
    "\n",
    "print(f\"Found {len(all_paths)} total images and corrected labels in memory.\")\n",
    "\n",
    "# --- Step 2: Identify All Unique Classes ---\n",
    "all_labels = [label for path, label in all_paths]\n",
    "unique_classes = sorted(list(set(all_labels)))\n",
    "RANDOM_SEED = 42  # For reproducible class splits\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(unique_classes)\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"Found {num_classes} unique classes.\")\n",
    "\n",
    "# --- Step 3: Split the *Classes* into Disjoint Sets ---\n",
    "# Class split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "# TEST_RATIO = 0.15 (implicit)\n",
    "\n",
    "train_split = int(TRAIN_RATIO * num_classes)\n",
    "val_split = int(VAL_RATIO * num_classes)\n",
    "\n",
    "train_classes = unique_classes[:train_split]\n",
    "val_classes = unique_classes[train_split : train_split + val_split]\n",
    "test_classes = unique_classes[train_split + val_split:]\n",
    "\n",
    "print(f\"Splitting classes into: {len(train_classes)} train, {len(val_classes)} validation, {len(test_classes)} test.\")\n",
    "\n",
    "# Sanity check: ensure the class sets are disjoint (no overlap)\n",
    "# Validate class splits are disjoint\n",
    "if len(set(train_classes) & set(val_classes)) > 0:\n",
    "    raise ValueError(\"Train and validation classes overlap!\")\n",
    "if len(set(train_classes) & set(test_classes)) > 0:\n",
    "    raise ValueError(\"Train and test classes overlap!\")\n",
    "if len(set(val_classes) & set(test_classes)) > 0:\n",
    "    raise ValueError(\"Validation and test classes overlap!\")\n",
    "print(\"Class splits are successfully disjoint.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Create Final Data Lists ---\n",
    "paths_train, paths_val, paths_test = [], [], []\n",
    "\n",
    "for path, label in all_paths:\n",
    "    if label in train_classes:\n",
    "        paths_train.append([path, label])\n",
    "    elif label in val_classes:\n",
    "        paths_val.append([path, label])\n",
    "    else:\n",
    "        paths_test.append([path, label])\n",
    "\n",
    "print(f\"Final data splits (number of images): Train={len(paths_train)}, Val={len(paths_val)}, Test={len(paths_test)}\")\n",
    "\n",
    "\n",
    "# --- Step 5: Write the new CSV files ---\n",
    "data = {'train': paths_train,\n",
    "        'test': paths_test,\n",
    "        'val': paths_val,\n",
    "}\n",
    "\n",
    "os.makedirs(\"/kaggle/working/materials\", exist_ok=True)\n",
    "csv_files = [\"train.csv\", \"test.csv\", \"val.csv\"]\n",
    "\n",
    "for fname in csv_files:\n",
    "    path = os.path.join(\"/kaggle/working/materials\", fname)\n",
    "    with open(path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"filename\", \"label\"])\n",
    "        writer.writerows(data[fname[:-4]])\n",
    "\n",
    "print(\"\\nCSV files with diverse and corrected data splits created successfully inside 'materials' folder.\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.04519Z",
     "iopub.execute_input": "2025-10-26T00:46:55.045444Z",
     "iopub.status.idle": "2025-10-26T00:46:55.443421Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.045423Z",
     "shell.execute_reply": "2025-10-26T00:46:55.442297Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# First, create the datasets\ntrainset = UiSmell('train', '/kaggle/input/plantvillage-dataset/color/', is_aug=True)\nvalset = UiSmell('val', '/kaggle/input/plantvillage-dataset/color/', is_aug=False)\ntestset = UiSmell('test', '/kaggle/input/plantvillage-dataset/color/', is_aug=False)\n\n# Now you can print the classes\nprint(\"=== All Classes and Their Labels ===\")\nfor class_name, label_id in trainset.label_map.items():\n    print(f\"Label {label_id}: {class_name}\")\n\nprint(\"\\n=== Classes in Order ===\")\nsorted_classes = sorted(trainset.label_map.items(), key=lambda x: x[1])\nfor class_name, label_id in sorted_classes:\n    print(f\"{label_id}: {class_name}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.444318Z",
     "iopub.execute_input": "2025-10-26T00:46:55.444606Z",
     "iopub.status.idle": "2025-10-26T00:46:55.515004Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.444581Z",
     "shell.execute_reply": "2025-10-26T00:46:55.514266Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# DATA LOADER\ntrainset = UiSmell('train', args['data-path'], is_aug=True)\ntrain_sampler = CategoriesSampler(trainset.label, args['num_sampler'],\n                                  args['train-way'], args['shot'] + args['query'])\ntrain_loader = DataLoader(dataset=trainset, batch_sampler=train_sampler, shuffle=False,)\n\nvalset = UiSmell('val', args['data-path'], is_aug=False)\nval_sampler = CategoriesSampler(valset.label, args['num_sampler'],\n                                args['test-way'], args['shot'] + args['query'])\nval_loader = DataLoader(dataset=valset, batch_sampler=val_sampler, shuffle=False,)\n\ntestset = UiSmell('test', args['data-path'], is_aug=False)\ntest_sampler = CategoriesSampler(testset.label, args['num_sampler'],\n                                args['test-way'], args['shot'] + args['query'])\ntest_loader = DataLoader(dataset=testset, batch_sampler=test_sampler, shuffle=False,)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.515693Z",
     "iopub.execute_input": "2025-10-26T00:46:55.515903Z",
     "iopub.status.idle": "2025-10-26T00:46:55.611087Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.515887Z",
     "shell.execute_reply": "2025-10-26T00:46:55.610385Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "label_map = trainset.label_map\nsorted_items = sorted(label_map.items(), key=lambda item: item[1])\n\n# Extract the class names in the now correct order\nclass_names_ordered = [item[0] for item in sorted_items]\n\n# Define the output path for the class names file\noutput_path = '/kaggle/working/materials/classes.txt'\n\n# Write the ordered class names to the file, one per line\nwith open(output_path, 'w') as f:\n    for name in class_names_ordered:\n        f.write(f\"{name}\\n\")\n\nprint(f\"Successfully created classes.txt at: {output_path}\")\nprint(\"\\\\n--- Class Names (in order) ---\")\nfor i, name in enumerate(class_names_ordered):\n    print(f\"{i}: {name}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.611846Z",
     "iopub.execute_input": "2025-10-26T00:46:55.61205Z",
     "iopub.status.idle": "2025-10-26T00:46:55.618354Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.612035Z",
     "shell.execute_reply": "2025-10-26T00:46:55.617652Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# run in your notebook\nprint(\"Using transform (is_aug):\", trainset.is_aug)\nsample_path = trainset.data[0]\nimport numpy as np\nfrom PIL import Image\nimg = np.array(Image.open(sample_path).convert('RGB'))\nout = trainset.transform(image=img)\nprint(\"transform output keys:\", out.keys())\nprint(\"image type from transform:\", type(out['image']), \"shape:\", getattr(out['image'], 'shape', None))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.619116Z",
     "iopub.execute_input": "2025-10-26T00:46:55.619382Z",
     "iopub.status.idle": "2025-10-26T00:46:55.647445Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.619357Z",
     "shell.execute_reply": "2025-10-26T00:46:55.646623Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nt = out['image']  # torch.Tensor C,H,W\nimg_vis = t.permute(1,2,0).cpu().numpy()\nmean = np.array([0.485,0.456,0.406])\nstd  = np.array([0.229,0.224,0.225])\nimg_vis = np.clip(img_vis * std + mean, 0, 1)\nplt.imshow(img_vis); plt.axis('off'); plt.title('Augmented image (what model sees)')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.648343Z",
     "iopub.execute_input": "2025-10-26T00:46:55.648605Z",
     "iopub.status.idle": "2025-10-26T00:46:55.759019Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.648582Z",
     "shell.execute_reply": "2025-10-26T00:46:55.758175Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"albumentations version:\", A.__version__)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.759726Z",
     "iopub.execute_input": "2025-10-26T00:46:55.759932Z",
     "iopub.status.idle": "2025-10-26T00:46:55.764707Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.759917Z",
     "shell.execute_reply": "2025-10-26T00:46:55.763861Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os, os.path as osp\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure save directory exists\n",
    "if not os.path.exists(args['save-path']):\n",
    "    os.makedirs(args['save-path'])\n",
    "\n",
    "def save_model(name: str) -> None:\n",
    "    \"\"\"Save model checkpoint to disk.\n",
    "\n",
    "    Args:\n",
    "        name: Checkpoint name (without .pth extension)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(args['save-path']):\n",
    "        os.makedirs(args['save-path'])\n",
    "    torch.save(model.state_dict(), osp.join(args['save-path'], name + '.pth'))\n",
    "\n",
    "# Optimizer / Scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Projection module\n",
    "# Creates class-specific subspaces for few-shot classification\n",
    "projection_pro = Subspace_Projection(num_dim=args['subspace-dim'], debug=False)\n",
    "\n",
    "# Training log\n",
    "trlog = {\n",
    "    'train_loss': [], 'val_loss': [], 'test_loss': [],\n",
    "    'train_acc': [], 'val_acc': [], 'test_acc': [],\n",
    "    'max_acc': 0.0, 'max_epoch': 0\n",
    "}\n",
    "\n",
    "timer = Timer()\n",
    "# Early stopping configuration\n",
    "patience = 3  # Stop if no improvement for this many epochs\n",
    "epochs_no_improve = 0  # Counter for epochs without improvement\n",
    "\n",
    "# MAIN TRAIN LOOP\n",
    "# Configuration:\n",
    "# - Model: ResNet50 with subspace projection\n",
    "# - Optimizer: Adam with lr={args[\"lr\"]}\n",
    "# - N-way: {args[\"train-way\"]}-way (train) / {args[\"test-way\"]}-way (test)\n",
    "# - K-shot: {args[\"shot\"]}-shot with {args[\"query\"]} query samples\n",
    "# - Subspace dimension: {args[\"subspace-dim\"]}\n",
    "# - Lambda (discriminative loss weight): {args[\"lamb\"]}\n",
    "\n",
    "for epoch in range(1, args['max-epoch'] + 1):\n",
    "    model.train()\n",
    "    shot_num = args['shot'] * 2 if args['shot'] == 1 else args['shot']\n",
    "\n",
    "    tl = Averager()\n",
    "    ta = Averager()\n",
    "\n",
    "    for i, batch in tqdm(enumerate(train_loader, 1)):\n",
    "        data, _ = [_.cuda() for _ in batch]\n",
    "\n",
    "        if i == 1 and epoch == 1:\n",
    "            print(f\"[DEBUG] Epoch {epoch}, Iter {i}\")\n",
    "            print(f\"data shape: {data.shape}\")\n",
    "            print(f\"train-way={args['train-way']}, shot={args['shot']}, query={args['query']}\")\n",
    "\n",
    "        p = args['shot'] * args['train-way']\n",
    "        qq = p + args['query'] * args['train-way']\n",
    "        data_shot, data_query = data[:p], data[p:qq]\n",
    "\n",
    "        if args['shot'] == 1:\n",
    "            data_shot = torch.cat((data_shot, flip(data_shot, 3)), dim=0)\n",
    "\n",
    "        # Forward pass: Generate prototypes from support set\n",
    "        proto = model(data_shot)\n",
    "        proto = proto.reshape(shot_num, args['train-way'], -1)\n",
    "        proto = torch.transpose(proto, 0, 1)\n",
    "        hyperplanes, mu = projection_pro.create_subspace(proto, args['train-way'], shot_num)\n",
    "\n",
    "        # Labels\n",
    "        label = torch.arange(args['train-way']).repeat(args['query'])\n",
    "        label = label.type(torch.cuda.LongTensor)\n",
    "\n",
    "        # Compute similarity to class subspaces\n",
    "        query_features = model(data_query)\n",
    "        logits, discriminative_loss = projection_pro.projection_metric(query_features, hyperplanes, mu=mu)\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, label)\n",
    "        loss = ce_loss + args['lamb'] * discriminative_loss\n",
    "        acc = count_acc(logits, label)\n",
    "\n",
    "        tl.add(loss.item())\n",
    "        ta.add(acc)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    print(f'epoch {epoch}, loss={tl.item():.4f} acc={ta.item():.4f}')\n",
    "\n",
    "    tl = tl.item()\n",
    "    ta = ta.item()\n",
    "\n",
    "    # Skip early validation to save time\n",
    "    if epoch % 2 != 0 and epoch < 100:\n",
    "        continue\n",
    "\n",
    "    # VALIDATION PHASE\n",
    "    model.eval()\n",
    "    vl = Averager()\n",
    "    va = Averager()\n",
    "\n",
    "    for i, batch in tqdm(enumerate(val_loader, 1)):\n",
    "        data, _ = [_.cuda() for _ in batch]\n",
    "        p = args['shot'] * args['test-way']\n",
    "        data_shot, data_query = data[:p], data[p:]\n",
    "\n",
    "        if args['shot'] == 1:\n",
    "            data_shot = torch.cat((data_shot, flip(data_shot, 3)), dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            proto = model(data_shot)\n",
    "            proto = proto.reshape(shot_num, args['test-way'], -1)\n",
    "            proto = torch.transpose(proto, 0, 1)\n",
    "            hyperplanes, mu = projection_pro.create_subspace(proto, args['test-way'], shot_num)\n",
    "            logits, _ = projection_pro.projection_metric(model(data_query), hyperplanes, mu=mu)\n",
    "\n",
    "        label = torch.arange(args['test-way']).repeat(args['query']).type(torch.cuda.LongTensor)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        acc = count_acc(logits, label)\n",
    "\n",
    "        vl.add(loss.item())\n",
    "        va.add(acc)\n",
    "\n",
    "    vl = vl.item()\n",
    "    va = va.item()\n",
    "\n",
    "    # Save best\n",
    "    if va > trlog['max_acc']:\n",
    "        trlog['max_acc'] = va\n",
    "        save_model('max-acc')\n",
    "        trlog['max_epoch'] = epoch\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    print(f'epoch {epoch}, val, loss={vl:.4f} acc={va:.4f} maxacc={trlog[\"max_acc\"]:.4f}')\n",
    "\n",
    "    trlog['train_loss'].append(tl)\n",
    "    trlog['train_acc'].append(ta)\n",
    "    trlog['val_loss'].append(vl)\n",
    "    trlog['val_acc'].append(va)\n",
    "\n",
    "    save_model('epoch-last')\n",
    "    if epoch % args['save-epoch'] == 0:\n",
    "        save_model(f'epoch-{epoch}')\n",
    "\n",
    "    print(f'ETA:{timer.measure()}/{timer.measure(epoch / args[\"max-epoch\"])}')\n",
    "\n",
    "    # TEST PHASE\n",
    "    tel = Averager()\n",
    "    tea = Averager()\n",
    "\n",
    "    for i, batch in tqdm(enumerate(test_loader, 1)):\n",
    "        data, _ = [_.cuda() for _ in batch]\n",
    "        p = args['shot'] * args['test-way']\n",
    "        data_shot, data_query = data[:p], data[p:]\n",
    "\n",
    "        if args['shot'] == 1:\n",
    "            data_shot = torch.cat((data_shot, flip(data_shot, 3)), dim=0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            proto = model(data_shot)\n",
    "            proto = proto.reshape(shot_num, args['test-way'], -1)\n",
    "            proto = torch.transpose(proto, 0, 1)\n",
    "            hyperplanes, mu = projection_pro.create_subspace(proto, args['test-way'], shot_num)\n",
    "            logits, _ = projection_pro.projection_metric(model(data_query), hyperplanes, mu=mu)\n",
    "\n",
    "        label = torch.arange(args['test-way']).repeat(args['query']).type(torch.cuda.LongTensor)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        acc = count_acc(logits, label)\n",
    "\n",
    "        tel.add(loss.item())\n",
    "        tea.add(acc)\n",
    "\n",
    "    tel = tel.item()\n",
    "    tea = tea.item()\n",
    "    print(f'epoch {epoch}, test, loss={tel:.4f} acc={tea:.4f} maxacc={trlog[\"max_acc\"]:.4f}')\n",
    "    \n",
    "    trlog['test_loss'].append(tel)\n",
    "    trlog['test_acc'].append(tea)\n",
    "\n",
    "    save_path = osp.join(args['save-path'], 'trlog.json')\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(trlog, f, indent=4)\n",
    "\n",
    "    print(f'TEST ETA:{timer.measure()}/{timer.measure(epoch / args[\"max-epoch\"])}')\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping triggered after {patience} epochs with no improvement.')\n",
    "        break"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T00:46:55.765584Z",
     "iopub.execute_input": "2025-10-26T00:46:55.765932Z",
     "iopub.status.idle": "2025-10-26T01:39:11.184817Z",
     "shell.execute_reply.started": "2025-10-26T00:46:55.765897Z",
     "shell.execute_reply": "2025-10-26T01:39:11.184073Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport json\nimport os.path as osp\n\n# Load trlog (if not already in memory)\nsave_path = osp.join(args['save-path'], 'trlog.json')\nwith open(save_path, 'r') as f:\n    trlog = json.load(f)\n\nepochs = range(1, len(trlog['train_loss']) + 1)\n\nplt.figure(figsize=(12, 5))\n\n# ----- LOSS -----\nplt.subplot(1, 2, 1)\nplt.plot(epochs, trlog['train_loss'], label='Train Loss')\nplt.plot(epochs, trlog['val_loss'], label='Val Loss')\nplt.plot(epochs, trlog['test_loss'], label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss over Epochs')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# ----- ACCURACY -----\nplt.subplot(1, 2, 2)\nplt.plot(epochs, trlog['train_acc'], label='Train Acc')\nplt.plot(epochs, trlog['val_acc'], label='Val Acc')\nplt.plot(epochs, trlog['test_acc'], label='Test Acc')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy over Epochs')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T01:39:11.185595Z",
     "iopub.execute_input": "2025-10-26T01:39:11.185862Z",
     "iopub.status.idle": "2025-10-26T01:39:11.591187Z",
     "shell.execute_reply.started": "2025-10-26T01:39:11.185837Z",
     "shell.execute_reply": "2025-10-26T01:39:11.590336Z"
    },
    "editable": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "save_model_path = '/kaggle/working/Resnet50_plant_disease_model_26102025.pth'\ntorch.save(model.state_dict(), save_model_path)\n\nprint(f\"Model telah disimpan di: {save_model_path}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-26T01:39:11.592196Z",
     "iopub.execute_input": "2025-10-26T01:39:11.592744Z",
     "iopub.status.idle": "2025-10-26T01:39:11.747336Z",
     "shell.execute_reply.started": "2025-10-26T01:39:11.592717Z",
     "shell.execute_reply": "2025-10-26T01:39:11.746511Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}